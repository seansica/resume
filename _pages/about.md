---
layout: about
title: About
permalink: /
subtitle: >
  AI Safety & Interpretability · <a href='https://www.mitre.org/'>MITRE</a> · <a href='https://www.ischool.berkeley.edu/programs/mids'>UC Berkeley MIDS</a>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false
  more_info:

selected_papers: false
social: true

announcements:
  enabled: false
  scrollable: true
  limit: 5

latest_posts:
  enabled: false
---

I’m a research engineer working at the intersection of **mechanistic interpretability**, **AI safety**, and **cybersecurity**.

At [MITRE](https://www.mitre.org/), I help lead software development for [MITRE ATT&CK](https://attack.mitre.org/) and contribute to research on [Judy](https://www.mitre.org/news-insights/impact-story/naming-supercomputer-mitre-honors-early-innovator), the supercomputing platform powering the [Federal AI Sandbox](https://www.mitre.org/focus-areas/artificial-intelligence/federal-ai-sandbox).

In 2025, I focused on sparse autoencoders, representation shift under fine-tuning, and building practical tooling for interpretability workflows. I contributed enterprise features to [Neuronpedia](https://www.neuronpedia.org/) and helped launch MITRE’s internal mechanistic interpretability initiative.

I bring 5+ years of experience shipping production software across distributed infrastructure, APIs, open-source tooling, and Kubernetes-based systems.

I’m currently enrolled in the [BlueDot Technical AI Safety Course](https://bluedot.org/courses/technical-ai-safety/) and am seeking research engineering roles in frontier AI labs. I’m especially interested in research agendas that use interpretability to enable guided or intentional training. I’m also interested in evaluation awareness and model misalignment.
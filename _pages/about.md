---
layout: about
title: about
permalink: /
subtitle: >
  AI Safety & Interpretability · <a href='https://www.mitre.org/'>MITRE</a> · <a href='https://www.ischool.berkeley.edu/programs/mids'>UC Berkeley MIDS</a>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false
  more_info:

selected_papers: true
social: true

announcements:
  enabled: true
  scrollable: true
  limit: 5

latest_posts:
  enabled: false
---

I'm interested in **mechanistic interpretability** and **AI safety** — understanding what neural networks learn and how to make them reliably safe.

I'm a Lead Software Engineer at [MITRE](https://www.mitre.org/), where I split my time between leading the [ATT&CK](https://attack.mitre.org/) software team and conducting interpretability research on MITRE's Federal AI Sandbox. My research focuses on sparse autoencoders, causal analysis of how fine-tuning shifts learned representations, and building tooling for scalable mechanistic interpretability workflows. I contributed enterprise features to [Neuronpedia](https://www.neuronpedia.org/) and bootstrapped MITRE's internal interpretability effort.

I'm enrolled in the [BlueDot Impact AI Safety Fundamentals](https://aisafetyfundamentals.com/) course and completing my **Master of Information and Data Science** at [UC Berkeley](https://www.ischool.berkeley.edu/programs/mids) (3.97 GPA, expected August 2025), where my coursework focused on NLP and generative AI. My [research](https://github.com/seansica/sae-vector-steering) examined the causal effects of fine-tuning on sparse autoencoder features using mechanistic interventions.

Before pivoting to ML research, I spent 5+ years shipping production software — datacenter infrastructure, REST APIs, open-source Python and TypeScript libraries used by thousands of organizations, and Kubernetes deployments. I hold a BS in Computer Science from [Boston University](https://www.bu.edu/).

I'm seeking **research engineering roles** at AI safety organizations where I can apply deep engineering experience to alignment and interpretability problems.
